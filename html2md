#!/usr/bin/env python3
import sys
import csv
import os
import datetime
import re
import tarfile
import requests
import shutil
from bs4 import BeautifulSoup, NavigableString, Tag
from urllib.parse import urlparse
from urllib.request import url2pathname

# 1. HTML Cleaning
def clean_soup(soup):
    content_div = soup.find('div', {'class': 'mw-parser-output'})
    if not content_div:
        return None

    for tag in content_div.find_all(['table', 'img', 'figure', 'script', 'style', 'math', 'iframe']):
        tag.decompose()

    classes_to_remove = [
        'navbox', 'infobox', 'mw-editsection', 'reference', 'reflist',
        'noprint', 'mw-citethispage', 'catlinks', 'thumb', 'toc',
        'hatnote', 'sistersitebox', 'ambox'
    ]

    for cls in classes_to_remove:
        for tag in content_div.find_all(class_=cls):
            tag.decompose()

    ids_to_remove = ['References', 'External_links', 'See_also', 'Notes']
    header_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']

    for section_id in ids_to_remove:
        tag = content_div.find(id=section_id)
        if tag:
            parent_header = tag.find_parent(header_tags)
            if parent_header:
                parent_header.decompose()
            else:
                tag.decompose()

    return content_div

# 2. Markdown Conversion
def convert_list(tag, depth=0):
    md_output = ""
    indent = "  " * depth
    is_ordered = (tag.name == 'ol')
    list_items = tag.find_all('li', recursive=False)

    for index, li in enumerate(list_items):
        marker = f"{index + 1}." if is_ordered else "-"
        item_content = ""
        for child in li.children:
            if child.name in ['ul', 'ol']:
                item_content += "\n" + convert_list(child, depth + 1)
            else:
                item_content += process_element(child)

        md_output += f"{indent}{marker} {item_content.strip()}\n"
    return md_output

def process_element(element):
    if isinstance(element, NavigableString):
        return element.string if element.string else ""
    if not isinstance(element, Tag):
        return ""

    tag_name = element.name
    if tag_name not in ['ul', 'ol']:
        content = "".join([process_element(child) for child in element.children])
    else:
        content = ""

    if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(tag_name[1])
        return f"\n{'#' * level} {content.strip()}\n\n"
    elif tag_name == 'p':
        stripped = content.strip()
        return f"{stripped}\n\n" if stripped else ""
    elif tag_name in ['b', 'strong']:
        return f"**{content}**" if content.strip() else ""
    elif tag_name in ['i', 'em']:
        return f"*{content}*" if content.strip() else ""
    elif tag_name == 'code':
        return f"`{element.get_text()}`"
    elif tag_name == 'a':
        href = element.get('href')
        if href:
            return f"[{content}]({href})"
        return content
    elif tag_name in ['ul', 'ol']:
        return convert_list(element) + "\n"
    elif tag_name == 'blockquote':
        lines = "".join([process_element(child) for child in element.children]).splitlines()
        quoted = "\n".join([f"> {line}" for line in lines if line.strip()])
        return f"{quoted}\n\n"
    elif tag_name == 'pre':
        return f"\n```\n{element.get_text()}\n```\n\n"
    else:
        return content

# 3. Helper Functions
def download_html(url):
    parsed = urlparse(url)
    if parsed.scheme == 'file' or parsed.scheme == '':
        local_path = url2pathname(parsed.path)

        if parsed.netloc and parsed.netloc != 'localhost':
            local_path = '\\\\' + parsed.netloc + local_path

        if not os.path.exists(local_path):
            raise FileNotFoundError(f"Local file not found: {local_path}")

        with open(local_path, 'r', encoding='utf-8', errors='replace') as f:
            return f.read()
    else:
        headers = {'User-Agent': 'Mozilla/5.0 (Education Purpose Script)'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text

def sanitize_filename(title):
    clean = title.lower()
    clean = re.sub(r'[^a-z0-9]', '_', clean)
    clean = re.sub(r'_+', '_', clean)
    clean = clean.strip('_')
    return f"{clean}.md"

# 4. Main Execution
def main():
    if len(sys.argv) != 3:
        sys.stderr.write(" Usage: ./html2md <csv_file> <output_dir>\n")
        sys.exit(1)

    csv_file = sys.argv[1]
    output_dir = sys.argv[2]

    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
        except OSError as e:

            sys.stderr.write(f"Error creating directory {output_dir}: {e}\n")
            sys.exit(1)

    temp_dir = os.path.join(output_dir, "temp_processing")
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)

    today = datetime.date.today()
    generated_files = []

    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            try:
                reader = csv.reader(f, delimiter='|')

                for row_num, row in enumerate(reader, 1):
                    if not row: continue
                    if len(row) != 3:
                        sys.stderr.write(f"Skipping Wrong line {row_num}: {row}\n")
                        continue

                    title, url, date_str = row[0].strip(), row[1].strip(), row[2].strip()

                    try:
                        target_date = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()
                        if target_date > today:
                            continue

                        html_content = download_html(url)

                        soup = BeautifulSoup(html_content, 'html.parser')
                        main_content = clean_soup(soup)

                        if not main_content:
                            sys.stderr.write(f"Warning: No main content {url}\n")
                            continue

                        markdown_text = f"<!-- URL: {url} -->\n"
                        markdown_text += f"# {title}\n\n"
                        markdown_text += process_element(main_content)

                        filename = sanitize_filename(title)
                        filepath = os.path.join(temp_dir, filename)

                        with open(filepath, 'w', encoding='utf-8') as md_file:
                            md_file.write(markdown_text)

                        generated_files.append(filename)
                        print(f"Converted: {title} -> {filename}")

                    except requests.exceptions.HTTPError as e:
                        status_code = e.response.status_code if e.response is not None else "Unknown"
                        sys.stderr.write(f"Error: {url} --> Status: {status_code}\n")
                        continue

                    except requests.exceptions.RequestException as e:
                        sys.stderr.write(f"Error: Network issue --> {url}: {e}\n")
                        continue

                    except Exception as e:
                        sys.stderr.write(f"Other Error processing {url}: {e}\n")
                        continue

            except csv.Error as e:
                sys.stderr.write(f"Error: CSV parsing failed --> {reader.line_num}: {e}\n")
                shutil.rmtree(temp_dir)
                sys.exit(1)

    except FileNotFoundError:
        sys.stderr.write(f"CSV not found: {csv_file}\n")
        shutil.rmtree(temp_dir)
        sys.exit(1)

    if generated_files:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        archive_name = f"{timestamp}.tar.gz"
        archive_path = os.path.join(output_dir, archive_name)
        with tarfile.open(archive_path, "w:gz") as tar:
            for filename in generated_files:
                full_path = os.path.join(temp_dir, filename)
                tar.add(full_path, arcname=filename)
        print(f"Archive created: {archive_path}")
    else:
        print("No files converted")

    # 5. Cleanup
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()